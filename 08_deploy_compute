#!/usr/bin/env bash
# 08_deploy_compute
# Deploy slurm runtime, munge key, slurmd defaults and unit to lci-compute* nodes.
# - Detects the head node's 192.* address (prefers 192.168.*)
# - Creates /etc/default/slurmd containing SLURMCTLD_HOST=<head_ip>
# - Rsyncs /opt/slurm, copies /etc/munge/munge.key, deploys slurmd.service (if present)
# - Fixes permissions and restarts munge + slurmd on each compute node
#
# Usage: run as root (or a user that can SSH to remote root), from the directory
# containing this script and optionally slurmd.service.
set -euo pipefail

### --- Configuration (tweak if needed) ---
HOSTS_FILE="/etc/hosts"              # file used to enumerate hosts
LOCAL_SLURM_DIR="/opt/slurm"         # directory to rsync to remote /opt/
LOCAL_MUNGE_KEY="/etc/munge/munge.key"
SSH_USER="root"                      # remote account (root recommended here)
SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=accept-new -o ConnectTimeout=10"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
SLURMD_UNIT_LOCAL="${SCRIPT_DIR}/slurmd.service"  # optional, deploy if present
THIS_SHORT="$(hostname -s)"
THIS_FQDN="$(hostname -f || true)"
TMPDIR="$(mktemp -d /tmp/08_deploy_compute.XXXXXX)"
UMASK_ORIG="$(umask)"
# ------------------------------------------------

trap 'rc=$?; umask "$UMASK_ORIG"; rm -rf "$TMPDIR"; exit $rc' EXIT

# Helper: log
log() { printf '%s %s\n' "$(date +"%F %T")" "$*"; }

# Basic sanity checks
if [[ ! -d "$LOCAL_SLURM_DIR" ]]; then
    log "ERROR: local slurm dir '$LOCAL_SLURM_DIR' does not exist."
    exit 1
fi
if [[ ! -f "$LOCAL_MUNGE_KEY" ]]; then
    log "ERROR: local munge key '$LOCAL_MUNGE_KEY' does not exist."
    exit 1
fi

# Gather compute hosts from /etc/hosts (any word beginning with lci-compute)
mapfile -t COMPUTE_HOSTS < <(awk '{
    for (i=2;i<=NF;i++) if ($i ~ /^lci-compute/) print $i
}' "$HOSTS_FILE" | sort -u)

if [[ ${#COMPUTE_HOSTS[@]} -eq 0 ]]; then
    log "ERROR: no lci-compute* hosts found in $HOSTS_FILE"
    exit 1
fi

log "Found compute hosts: ${COMPUTE_HOSTS[*]}"

# --- Robust HEAD_IP detection ---
# Collect candidate head hostnames from hosts file (words starting with lci-head)
mapfile -t HEAD_NAMES < <(awk '{
    for (i=2;i<=NF;i++) if ($i ~ /^lci-head/) print $i
}' "$HOSTS_FILE" | sort -u)

if [[ ${#HEAD_NAMES[@]} -eq 0 ]]; then
    log "ERROR: no hostnames beginning with 'lci-head' found in $HOSTS_FILE"
    exit 1
fi

log "Detected head names in $HOSTS_FILE: ${HEAD_NAMES[*]}"

# Helper function: find a full hosts-file line that starts with a given IP regex
_find_line_for_pattern() {
    local pat="$1"   # awk regex for $1 (IP) e.g. '^192\.168\.'
    for hn in "${HEAD_NAMES[@]}"; do
        # look for hosts-file line whose IP matches pat and contains hn as a word
        line="$(awk -v pat="$pat" -v name="$hn" '
            $1 ~ pat {
                for (i=2;i<=NF;i++) if ($i == name) { print; exit }
            }
        ' "$HOSTS_FILE" || true)"
        if [[ -n "$line" ]]; then
            printf '%s' "$line"
            return 0
        fi
    done
    return 1
}

HEAD_LINE=""
# 1) prefer 192.168.*
if line="$(_find_line_for_pattern '^192\\.168\\.')"; then
    HEAD_LINE="$line"
else
    # 2) any 192.*
    if line="$(_find_line_for_pattern '^192\\.')" ; then
        HEAD_LINE="$line"
    else
        # 3) fallback: any hosts-file line that contains a lci-head* entry (first match)
        HEAD_LINE="$(awk '{
            for (i=2;i<=NF;i++) if ($i ~ /^lci-head/) { print; exit }
        }' "$HOSTS_FILE" || true)"
    fi
fi

if [[ -z "$HEAD_LINE" ]]; then
    log "ERROR: could not find any /etc/hosts line for lci-head* in $HOSTS_FILE"
    exit 1
fi

# Extract the IP (first whitespace-separated token)
HEAD_IP="$(awk '{print $1; exit}' <<<"$HEAD_LINE")" || true

log "Chosen hosts-file line for SLURMCTLD_HOST: $HEAD_LINE"
log "Using head IP for SLURMCTLD_HOST: $HEAD_IP"

# Create temporary defaults file (to be copied to each compute node)
TMP_DEFAULTS="$TMPDIR/slurmd_defaults"
cat > "$TMP_DEFAULTS" <<EOF
# /etc/default/slurmd - created by 08_deploy_compute
# Contains environment variables consumed by systemd unit EnvironmentFile
SLURMCTLD_HOST=${HEAD_IP}
EOF
chmod 0644 "$TMP_DEFAULTS"
log "Local defaults file created at $TMP_DEFAULTS"

# If local slurmd.service exists, make it available to copy
if [[ -f "$SLURMD_UNIT_LOCAL" ]]; then
    log "Found local slurmd.service at $SLURMD_UNIT_LOCAL — it will be deployed to compute nodes."
else
    log "No local slurmd.service found at $SLURMD_UNIT_LOCAL — skipping unit deployment (assuming unit already on remote)."
fi

# Iterate compute nodes
for host in "${COMPUTE_HOSTS[@]}"; do
    # skip the head node (if listed among compute hosts)
    if [[ "$host" == "$THIS_SHORT" || "$host" == "$THIS_FQDN" || "$host" == "$(hostname -A 2>/dev/null | awk '{print $1}')" ]]; then
        log "Skipping local host: $host"
        continue
    fi

    log "=== Processing $host ==="

    # 1) rsync slurm -> /opt/ on remote
    log "-> rsync $LOCAL_SLURM_DIR -> ${SSH_USER}@${host}:/opt/"
    if ! rsync -aHAX --delete --progress -e "ssh $SSH_OPTS" "$LOCAL_SLURM_DIR" "${SSH_USER}@${host}:/opt/"; then
        log "ERROR: rsync to $host failed — skipping host"
        continue
    fi

    # 2) copy munge key
    log "-> copying munge key -> ${SSH_USER}@${host}:/etc/munge/munge.key"
    if ! rsync -a --progress -e "ssh $SSH_OPTS" "$LOCAL_MUNGE_KEY" "${SSH_USER}@${host}:/etc/munge/munge.key"; then
        log "ERROR: munge key copy to $host failed — skipping host"
        continue
    fi

    # 3) copy slurmd defaults to /etc/default/slurmd
    log "-> deploying defaults -> ${SSH_USER}@${host}:/etc/default/slurmd"
    if ! rsync -a --progress -e "ssh $SSH_OPTS" "$TMP_DEFAULTS" "${SSH_USER}@${host}:/etc/default/slurmd"; then
        log "ERROR: defaults file copy to $host failed — skipping host"
        continue
    fi

    # 4) copy slurmd.service if present
    if [[ -f "$SLURMD_UNIT_LOCAL" ]]; then
        log "-> deploying slurmd.service -> ${SSH_USER}@${host}:/etc/systemd/system/slurmd.service"
        if ! rsync -a --progress -e "ssh $SSH_OPTS" "$SLURMD_UNIT_LOCAL" "${SSH_USER}@${host}:/etc/systemd/system/slurmd.service"; then
            log "WARNING: slurmd.service copy to $host failed — continuing with other steps"
        else
            # ensure unit file proper perms on remote later
            unit_deployed=true
        fi
    else
        unit_deployed=false
    fi

    # 5) remote fix perms, verify md5, enable/reload/restart services
    log "-> setting remote permissions, verifying md5, and restarting munge + slurmd on $host"
    # Use a heredoc for remote commands; keep it POSIX-ish and robust
    ssh $SSH_OPTS "${SSH_USER}@${host}" bash -eux <<'REMOTE'
MUNGE_PATH="/etc/munge/munge.key"
DEFAULTS_PATH="/etc/default/slurmd"
UNIT_PATH="/etc/systemd/system/slurmd.service"

# 1) munge key perms and restart
if [ -f "$MUNGE_PATH" ]; then
    chown munge:munge "$MUNGE_PATH" || true
    chmod 0400 "$MUNGE_PATH" || true
    echo "Remote md5 for munge key:"
    if command -v md5sum >/dev/null 2>&1; then
        md5sum "$MUNGE_PATH"
    else
        sha256sum "$MUNGE_PATH" || true
    fi
    echo "Restarting munge service..."
    systemctl restart munge || { systemctl status munge --no-pager -l || true; exit 1; }
    systemctl is-active --quiet munge && echo "munge active" || { echo "munge failed to start" >&2; systemctl status munge --no-pager -l || true; exit 1; }
else
    echo "remote munge key not found: $MUNGE_PATH" >&2
    exit 2
fi

# 2) defaults file perms and show contents
if [ -f "$DEFAULTS_PATH" ]; then
    chown root:root "$DEFAULTS_PATH" || true
    chmod 0644 "$DEFAULTS_PATH" || true
    echo "Remote contents of $DEFAULTS_PATH:"
    sed -n '1,200p' "$DEFAULTS_PATH" || true
else
    echo "remote defaults file not found: $DEFAULTS_PATH" >&2
    exit 3
fi

# 3) unit file perms if present
if [ -f "$UNIT_PATH" ]; then
    chown root:root "$UNIT_PATH" || true
    chmod 0644 "$UNIT_PATH" || true
fi

# 4) systemd reload & slurmd restart
echo "Daemon-reload, restart slurmd..."
systemctl daemon-reload || true
# enable the unit if present
if [ -f "$UNIT_PATH" ]; then
    systemctl enable --now slurmd || true
else
    # try to restart anyway (unit may exist already on remote)
    systemctl restart slurmd || true
fi
systemctl is-active --quiet slurmd && echo "slurmd active" || { echo "slurmd failed to start" >&2; systemctl status slurmd --no-pager -l || true; exit 4; }
REMOTE

    # 6) show local md5 for sanity
    log "Local md5 of munge key:"
    if command -v md5sum >/dev/null 2>&1; then
        md5sum "$LOCAL_MUNGE_KEY" || true
    else
        sha256sum "$LOCAL_MUNGE_KEY" || true
    fi

    log "Done with $host"
    echo
done

# cleanup (trap will remove TMPDIR)
umask "$UMASK_ORIG"
log "All done."
